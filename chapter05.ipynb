{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Hands On: Neural Networks in Python\n",
    "There are many tool kits that support implementation of neural networks,\n",
    "often based on Python, currently the most popular scripting language.\n",
    "\n",
    "In the following chapters, I will go over how to implement neural\n",
    "machine translation models. You can type the following commands in\n",
    "the interactive Python interpreter and inspect what they compute.\n",
    "\n",
    "### 5.7.1 Data Structures and Functions in Numpy\n",
    "Let us start with the inference and training I described in this chapter.\n",
    "For now, we do not use any dedicated neural network tool kit. However,\n",
    "we do the advanced math library numpy, which supports computation\n",
    "with vectors, matrices, and other tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer science, the typical data structure to represent vectors,\n",
    "matrices, and higher-order tensors is an array. Numpy has its own array\n",
    "data structures for which it defines basic tensor operations such as\n",
    "addition and multiplication.\n",
    "\n",
    "Here is how we represent the parameters for our example feed-forward neural network, which computes xor, i.e., the weight matrices\n",
    "W and W<sub>2</sub> and the bias vectors b and b<sub>2</sub>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "W = np.array([[3, 4], [2, 3]])\n",
    "b = np.array([-2, -4])\n",
    "W2 = np.array([5, -5])\n",
    "b2 = np.array([-2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid that we use as activation function is not already\n",
    "provided by Numpy, so we need to define it ourselves. The function\n",
    "operates element-wise on vectors, so we need to signal this to numpy\n",
    "with @np.vectorize. We define the sigmoid(x) function and it's derivative sigmoid'(x)\n",
    "\n",
    "> $sigmoid(x) = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "@np.vectorize\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output pair from our example was (1,0) → 1. So, we\n",
    "need to define these as well as vectors, thus as numpy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 0])\n",
    "t = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.2 Forward Computation\n",
    "Now, we have all the pieces in place to carry out inference in our neural\n",
    "network. Using matrix and vector representations the computation from\n",
    "the input to the hidden layer is\n",
    "\n",
    "> $sigmoid(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "> $s = Wx + b$\n",
    "\n",
    "> $h = sigmoid(s)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5.35)\n",
    "\n",
    "\n",
    "The computation from the hidden layer to the output layer is:\n",
    "\n",
    "> $z = W_{2}h +b$\n",
    "\n",
    "> $y = sigmoid(z)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5.36)\n",
    "\n",
    "Numpy makes it very easy to translate this into Python code. For the\n",
    "multiplication of a weight matrix with a vector, we need the dot product.\n",
    "Note that the default multiplication (*) is performed element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s = W.dot(x) + b\n",
    "h = sigmoid(s)\n",
    "\n",
    "z = W2.dot(h) + b2\n",
    "y = sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the value of the computed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7425526])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n",
    "# expected: array([0.7425526])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3 Backward Computation\n",
    "The next step is training via back-propagation. Recall that we start with\n",
    "the computed error and compute gradients of the weights with respect to\n",
    "the error. These gradients are then scaled with a learning rate and used\n",
    "to update parameters.\n",
    "\n",
    "So, first we need to compute the error between the computed output\n",
    "y and the target value t. We used the L2 norm for this, i.e., $E = \\frac{1}2{(t - y)^{2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "error = 1/2 * (t - y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to set the learning rate μ. This is typically a small\n",
    "number like 0.001 but here we just use 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mu = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went over quite a bit of math to derive formulas for the gradients\n",
    "$\\frac{\\partial E}{\\partial W},\n",
    "\\frac{\\partial E}{\\partial b},\n",
    "\\frac{\\partial E}{\\partial W_{2}}, \\text { and } \\frac{\\partial E}{\\partial b_{2}}$.\n",
    "Our update formulas simplified this a bit by first\n",
    "computing an error terms δ2 and δ1 and then using them for the weight\n",
    "updates.\n",
    "\n",
    "For the updates of parameters between the hidden layer and the final\n",
    "layer, our formulas first compute δ<sub>2</sub>, and then the weight updates for W<sub>2</sub>\n",
    "and b<sub>2</sub> are\n",
    "\n",
    "> \\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\delta_{2} &=(t-y) \\operatorname{sigmoid}^{\\prime}(z) \\\\\n",
    "\\Delta W_{2} &=\\mu \\delta_{2} h \\\\\n",
    "\\Delta b_{2} &=\\mu \\delta_{2}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Each equation can be formulated as one line in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "delta_2 = ( t - y ) * sigmoid_derivative( z )\n",
    "delta_W2 = mu * delta_2 * h\n",
    "delta_b2 = mu * delta_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update formulas for the parameters connecting the input layer\n",
    "to the hidden layer are quite similar, partly due to our introduction of the\n",
    "concept of an error term δ. The value for the this error term δ1 for first\n",
    "layer is computed partly based on the value for the error term δ2 for the\n",
    "second layer. This is back-propagation of the error in action:\n",
    "\n",
    ">$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\delta_{1} &=W \\; \\delta_{2} \\cdot \\operatorname{sigmoid}^{\\prime}(s) \\\\\n",
    "\\Delta W &=\\mu \\; \\delta_{1} \\; h \\\\\n",
    "\\Delta b &=\\mu \\; \\delta_{1}\n",
    "\\end{aligned}\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the Python code is quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "delta_1 = delta_2 * W2 * sigmoid_derivative( s )\n",
    "delta_W = mu * np.array([ delta_1 ]).T * x\n",
    "delta_b = mu * delta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.4 Repeated Use of the Chain Rule\n",
    "Let us take another view at the backward pass, which will also serve\n",
    "as a segue to the next chapter. You may have already forgotten how we\n",
    "derived at the formulas for the weight updates, and your head may be\n",
    "hurting from the flashbacks to high school math.\n",
    "\n",
    "However, it is actually simpler than you may believe. To derive\n",
    "weight updates through back-propagation, we rely heavily on the chain\n",
    "rule. Viewed from a high level, we have a computation chain, which is\n",
    "a sequence of function applications, such as matrix multiplication and\n",
    "activation functions.\n",
    "\n",
    "Consider the part of the calculation that connects the target output\n",
    "vector t, hidden vector h, the weight matrix W2, and the bias term b2 to\n",
    "the error E:\n",
    "\n",
    ">$\\begin{equation}\n",
    "E=\\mathrm{L} 2\\left(\\operatorname{sigmoid}\\left(W_{2} h+b_{2}\\right), t\\right)\n",
    "\\end{equation}$\n",
    "\n",
    "Consider the computation chain from the parameter matrix W<sub>2</sub> to\n",
    "E. We first have a matrix multiplication, then a vector addition, then the\n",
    "sigmoid and finally the L2 norm. To compute \\begin{equation}\n",
    "\\frac{\\partial E}{\\partial W_{2}}\n",
    "\\end{equation}, we treat the other values $(t, b2, h)$ as constants.\n",
    "Abstractly, we have a computation of the\n",
    "form $y = f(g(h(i(x))))$, with the weight matrix W<sub>2</sub> as input value x and\n",
    "the error $E$ as output value $y$.\n",
    "\n",
    "To obtain the derivative, our go-to rule here is the chain rule. Let us\n",
    "play this through for the simpler example of just two chained functions\n",
    "$f(g(x))$:\n",
    "\n",
    ">\\begin{equation}\n",
    "\\begin{aligned}\n",
    "F(x) &=f(g(x)) \\\\\n",
    "F^{\\prime}(x) &=f^{\\prime}(g(x)) g^{\\prime}(x)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Using different notation to clarify what we take the derivative over:\n",
    "\n",
    ">\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x} f(g(x))=\\frac{\\partial}{\\partial g(x)} f(g(x)) \\frac{\\partial}{\\partial x} g(x) .\n",
    "\\end{equation}\n",
    "\n",
    "Let us be explicit about the intermediate variable g(x) by naming it $a$:\n",
    "\n",
    ">\\begin{equation}\n",
    "\\begin{aligned}\n",
    "a &=g(x) \\\\\n",
    "\\frac{\\partial}{\\partial x} f(g(x)) &=\\frac{\\partial}{\\partial a} f(a) \\frac{\\partial}{\\partial x} a\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "So, we need to compute the derivatives of the elementary functions\n",
    "$f$ and $g$ with respect to their inputs. We also need the values computed in\n",
    "the forward computation since we will plug them into these derivatives.\n",
    "Let us now work through the computation chain, starting from the\n",
    "end. The last computation is the computation of the error:\n",
    "\n",
    ">\\begin{equation}\n",
    "\\begin{aligned}\n",
    "E(y) &=\\frac{1}{2}(t-y)^{2} \\\\\n",
    "\\frac{\\partial}{\\partial y} E(y) &=t-y .\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Let us do this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_error_d_y = t - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue on to the next step backward, the sigmoid activation function.\n",
    "It processed the intermediate value $z$. Since we are trying to compute\n",
    "gradients backward, we are still interested in the derivative of the error\n",
    "with respect to this value. So, this is where we now deploy the chain\n",
    "rule. Refer to Equation 5.42 with the substitutions $y → a$, $z → x$,\n",
    "$sigmoid → g$, and $E → f$ in mind:\n",
    "\n",
    ">\\begin{equation}\n",
    "\\begin{aligned}\n",
    "y &=\\operatorname{sigmoid}(z) \\\\\n",
    "\\frac{\\partial}{\\partial z} y &=\\operatorname{sigmoid}^{\\prime}(z) \\\\\n",
    "\\frac{\\partial}{\\partial z} E(\\operatorname{sigmoid}(z)) &=\\frac{\\partial}{\\partial y} E(y) \\frac{\\partial}{\\partial z} y\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We already computed \\begin{equation}\\frac{\\partial}{\\partial y} E(y)\\end{equation} in the previous step. So, we reuse that\n",
    "and multiply it with the derivative of the sigmoid, applied to the input\n",
    "value $z$ at this step.\n",
    "\n",
    "This is how it looks in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_y_d_z = sigmoid_derivative( z )\n",
    "d_error_d_z = d_error_d_y * d_y_d_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a closer look at one more step to get a good feeling for how\n",
    "the gradient computation progresses backward through the computation\n",
    "chain. The computation that computes $z$ is \\begin{equation}W_{2} h+b_{2}\\end{equation}. Here we are\n",
    "interested in several gradients. One each for the parameters $W_{2}$ and $b_{2}$\n",
    "to apply weight updates and one for the hidden layer values h to proceed\n",
    "with the backward computation.\n",
    "\n",
    "Let us look at just the derivatives for $W_{2}$:\n",
    "\n",
    ">\\begin{equation}\n",
    "\\begin{aligned}\n",
    "z &=W_{2} h+b_{2} \\\\\n",
    "\\frac{\\partial}{\\partial W_{2}} z &=h \\\\\n",
    "\\frac{\\partial}{\\partial W_{2}} E(\\operatorname{sigmoid}(z)) &=\\frac{\\partial}{\\partial z} E(\\operatorname{sigmoid}(z)) \\frac{\\partial}{\\partial W_{2}} z\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Again, we already computed \\begin{equation}\\frac{\\partial}{\\partial z} E(\\operatorname{sigmoid}(z))\\end{equation} in the previous step, so\n",
    "we can reuse it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_z_d_W2 = h\n",
    "d_error_d_W2 = d_error_d_z * d_z_d_W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient <code>d_error_d_W2</code> that we computed at this point\n",
    "matches <code>delta_W2</code> (see the previous section) since the learning rate $μ$\n",
    "is 1. We can check this by comparing the computed values by our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03597961, 0.00586666])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_error_d_W2\n",
    "# expected: array([0.03597961, 0.00586666])\n",
    "delta_W2\n",
    "# expected: array([0.03597961, 0.00586666])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computations for the other gradients for the forward step\n",
    "$z = W_{2}h + b_{2}$ follow the same logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_z_d_b2 = 1\n",
    "d_error_d_b2 = d_error_d_z * d_z_d_b2\n",
    "d_z_d_h = W2\n",
    "d_error_d_h = d_error_d_z * d_z_d_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computations for the layer connecting the input x to the hidden\n",
    "values $h$ matches closely with what we just presented in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_s_d_h = sigmoid_derivative( s )\n",
    "d_error_d_s = d_error_d_h * d_s_d_h\n",
    "d_W_d_s = x\n",
    "d_error_d_W = np.array([ d_error_d_s ]).T * d_W_d_s\n",
    "d_b_d_s = 1\n",
    "d_error_d_b = d_error_d_s * d_b_d_s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}